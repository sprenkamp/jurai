{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1mINFO    Running LLM\u001b[0m\n",
      "> \u001b[1mINFO    Params: Namespace(add_eos_token=False, apply_chat_template=False, auto_find_batch_size=False, backend='local-cli', batch_size=2, block_size=512, data_path='data/', deploy=False, disable_gradient_checkpointing=False, dpo_beta=0.1, epochs=4, evaluation_strategy='epoch', func=<function run_llm_command_factory at 0x7f35d873d9d0>, gradient_accumulation=4, inference=False, log='none', logging_steps=-1, lora_alpha=32, lora_dropout=0.045, lora_r=16, lr=2e-05, max_grad_norm=1.0, merge_adapter=False, mixed_precision=None, model='jphme/em_german_leo_mistral', model_max_length=1024, model_ref=None, optimizer='adamw_torch', padding=None, peft=False, project_name='JurAIGesetze', prompt_text_column='prompt', push_to_hub=False, quantization=None, rejected_text_column='rejected', repo_id=None, save_strategy='epoch', save_total_limit=1, scheduler='linear', seed=42, target_modules=None, text_column='answer', token=None, train=True, train_split='train', trainer='default', use_flash_attention_2=False, username=None, valid_split=None, version=False, warmup_ratio=0.1, weight_decay=0.01)\u001b[0m\n",
      "> \u001b[1mINFO    Dataset: JurAIGesetze (lm_training)\n",
      "Train data: ['data//train.csv']\n",
      "Valid data: []\n",
      "Column mapping: {'text': 'answer', 'rejected_text': 'rejected', 'prompt': 'prompt'}\n",
      "\u001b[0m\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆ| 122/122 [00:00<00:00, 52375.14 examples\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆ| 122/122 [00:00<00:00, 64986.68 examples\n",
      "> \u001b[1mINFO    Starting local training...\u001b[0m\n",
      "> \u001b[1mINFO    {\"model\":\"jphme/em_german_leo_mistral\",\"project_name\":\"JurAIGesetze\",\"data_path\":\"JurAIGesetze/autotrain-data\",\"train_split\":\"train\",\"valid_split\":null,\"add_eos_token\":false,\"block_size\":512,\"model_max_length\":1024,\"padding\":null,\"trainer\":\"default\",\"use_flash_attention_2\":false,\"log\":\"none\",\"disable_gradient_checkpointing\":false,\"logging_steps\":-1,\"evaluation_strategy\":\"epoch\",\"save_total_limit\":1,\"save_strategy\":\"epoch\",\"auto_find_batch_size\":false,\"mixed_precision\":null,\"lr\":0.00002,\"epochs\":4,\"batch_size\":2,\"warmup_ratio\":0.1,\"gradient_accumulation\":4,\"optimizer\":\"adamw_torch\",\"scheduler\":\"linear\",\"weight_decay\":0.01,\"max_grad_norm\":1.0,\"seed\":42,\"apply_chat_template\":false,\"quantization\":null,\"target_modules\":null,\"merge_adapter\":false,\"peft\":false,\"lora_r\":16,\"lora_alpha\":32,\"lora_dropout\":0.045,\"model_ref\":null,\"dpo_beta\":0.1,\"prompt_text_column\":\"autotrain_prompt\",\"text_column\":\"autotrain_text\",\"rejected_text_column\":\"autotrain_rejected_text\",\"push_to_hub\":false,\"repo_id\":null,\"username\":null,\"token\":null}\u001b[0m\n",
      "> \u001b[1mINFO    ['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'no', '-m', 'autotrain.trainers.clm', '--training_config', 'JurAIGesetze/training_params.json']\u001b[0m\n",
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n",
      "\u001b[1mðŸš€ INFO  \u001b[0m | \u001b[32m2024-01-16 15:45:03\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mloading dataset from disk\u001b[0m\n",
      "\u001b[1mðŸš€ INFO  \u001b[0m | \u001b[32m2024-01-16 15:45:03\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mTrain data: Dataset({\n",
      "    features: ['question', 'autotrain_text'],\n",
      "    num_rows: 122\n",
      "})\u001b[0m\n",
      "\u001b[1mðŸš€ INFO  \u001b[0m | \u001b[32m2024-01-16 15:45:03\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mValid data: None\u001b[0m\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.02s/it]\n",
      "\u001b[1mðŸš€ INFO  \u001b[0m | \u001b[32m2024-01-16 15:45:13\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m277\u001b[0m - \u001b[1mUsing block size 512\u001b[0m\n",
      "Running tokenizer on train dataset: 100%|â–ˆ| 122/122 [00:00<00:00, 5717.69 exampl\n",
      "Grouping texts in chunks of 512 (num_proc=4): 100%|â–ˆ| 122/122 [00:00<00:00, 320.\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "\u001b[1mðŸš€ INFO  \u001b[0m | \u001b[32m2024-01-16 15:45:14\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m339\u001b[0m - \u001b[1mcreating trainer\u001b[0m\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "\u001b[31m\u001b[1mâŒ ERROR \u001b[0m | \u001b[32m2024-01-16 15:45:17\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m90\u001b[0m - \u001b[31m\u001b[1mtrain has failed due to an exception: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/autotrain/trainers/common.py\", line 87, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/autotrain/trainers/clm/__main__.py\", line 403, in train\n",
      "    trainer = Trainer(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/transformers/trainer.py\", line 456, in __init__\n",
      "    self._move_model_to_device(model, args.device)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/transformers/trainer.py\", line 690, in _move_model_to_device\n",
      "    model = model.to(device)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 2460, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1160, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacty of 14.58 GiB of which 82.56 MiB is free. Including non-PyTorch memory, this process has 14.49 GiB memory in use. Of the allocated memory 14.40 GiB is allocated by PyTorch, and 1.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\u001b[0m\n",
      "\u001b[31m\u001b[1mâŒ ERROR \u001b[0m | \u001b[32m2024-01-16 15:45:17\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m91\u001b[0m - \u001b[31m\u001b[1mCUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacty of 14.58 GiB of which 82.56 MiB is free. Including non-PyTorch memory, this process has 14.49 GiB memory in use. Of the allocated memory 14.40 GiB is allocated by PyTorch, and 1.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"../config/first_train.yaml\", \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "hf_token = \"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\"\n",
    "\n",
    "import os\n",
    "os.environ[\"PROJECT_NAME\"] = config[\"project_name\"]\n",
    "os.environ[\"MODEL_NAME\"] = config[\"model_name\"]\n",
    "os.environ[\"PUSH_TO_HUB\"] = config[\"push_to_hub\"]\n",
    "os.environ[\"REPO_ID\"] = config[\"repo_id\"]\n",
    "os.environ[\"LEARNING_RATE\"] = config[\"learning_rate\"]\n",
    "os.environ[\"NUM_EPOCHS\"] = config[\"num_epochs\"]\n",
    "os.environ[\"BATCH_SIZE\"] = config[\"batch_size\"]\n",
    "os.environ[\"BLOCK_SIZE\"] = config[\"block_size\"]\n",
    "os.environ[\"WARMUP_RATIO\"] = config[\"warmup_ratio\"]\n",
    "os.environ[\"WEIGHT_DECAY\"] = config[\"weight_decay\"]\n",
    "os.environ[\"GRADIENT_ACCUMULATION\"] = config[\"gradient_accumulation\"]\n",
    "os.environ[\"USE_FP16\"] = config[\"use_fp16\"]\n",
    "os.environ[\"USE_PEFT\"] = config[\"use_peft\"]\n",
    "os.environ[\"USE_INT4\"] = config[\"use_int4\"]\n",
    "os.environ[\"LORA_R\"] = config[\"lora_r\"]\n",
    "os.environ[\"LORA_ALPHA\"] = config[\"lora_alpha\"]\n",
    "os.environ[\"LORA_DROPOUT\"] = config[\"lora_dropout\"]\n",
    "\n",
    "\n",
    "\n",
    "!autotrain llm \\\n",
    "--train \\\n",
    "--model ${MODEL_NAME} \\\n",
    "--project-name ${PROJECT_NAME} \\\n",
    "--data-path data/ \\\n",
    "--text-column answer \\\n",
    "--lr ${LEARNING_RATE} \\\n",
    "--batch-size ${BATCH_SIZE} \\\n",
    "--epochs ${NUM_EPOCHS} \\\n",
    "--block-size ${BLOCK_SIZE} \\\n",
    "--warmup-ratio ${WARMUP_RATIO} \\\n",
    "--lora-r ${LORA_R} \\\n",
    "--lora-alpha ${LORA_ALPHA} \\\n",
    "--lora-dropout ${LORA_DROPOUT} \\\n",
    "--weight-decay ${WEIGHT_DECAY} \\\n",
    "--gradient-accumulation ${GRADIENT_ACCUMULATION} \\\n",
    "$( [[ \"$USE_PEFT\" == \"True\" ]] && echo \"--use-peft\" ) \\\n",
    "$( [[ \"$PUSH_TO_HUB\" == \"True\" ]] && echo \"--push-to-hub --token ${HF_TOKEN} --repo-id ${REPO_ID}\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
